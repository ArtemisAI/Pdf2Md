# üöÄ Pdf2Md MCP Server Setup Instructions

## Complete Environment Setup for GitHub Copilot & GPU Acceleration

### Overview
This guide provides step-by-step instructions for setting up the Pdf2Md MCP Server with:
- ‚úÖ **GitHub Copilot environment compatibility** 
- ‚úÖ **GPU-accelerated audio transcription** (19.4x real-time performance)
- ‚úÖ **CPU fallback support** for universal compatibility
- ‚úÖ **Cross-platform support** (Windows/Linux)

---

## üîß Prerequisites

### System Requirements
- **Node.js**: v18+ (tested with v22.17.0)
- **Python**: 3.11+ (tested with 3.11.9)
- **PNPM**: Latest version
- **UV**: Python package manager (auto-installed)

### GPU Requirements (Optional but Recommended)
- **NVIDIA GPU**: GTX 1060 or newer
- **CUDA**: 12.1 or compatible
- **VRAM**: 2GB minimum (RTX 3060 with 12GB tested)

---

## üì¶ Installation

### 1. Clone Repository
```bash
git clone https://github.com/ArtemisAI/Pdf2Md.git
cd Pdf2Md
```

### 2. Install Dependencies

#### TypeScript/Node.js Dependencies
```bash
pnpm install
```

#### Python Dependencies (Base)
```bash
# Base dependencies for CPU processing
uv sync
```

#### GPU Dependencies (Recommended for Audio)
```bash
# Install CUDA-enabled PyTorch first
uv pip install torch==2.4.0+cu121 torchaudio==2.4.0+cu121 --index-url https://download.pytorch.org/whl/cu121

# Install GPU extras
uv sync --extra gpu

# Fix NumPy compatibility
uv pip install "numpy<2.0.0"
```

### 3. Verify Installation

#### Test CUDA Support
```bash
python -c "import torch; print('CUDA available:', torch.cuda.is_available()); print('GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU only')"
```

#### Test Faster-Whisper GPU
```bash
python -c "from faster_whisper import WhisperModel; model = WhisperModel('tiny', device='cuda' if __import__('torch').cuda.is_available() else 'cpu'); print('Whisper GPU ready!' if __import__('torch').cuda.is_available() else 'Whisper CPU ready!')"
```

#### Build MCP Server
```bash
pnpm run build
```

---

## üåê GitHub Copilot Environment Setup

### Environment Variables Configuration

1. **Navigate to GitHub Repository Settings**
   - Go to your repository ‚Üí Settings ‚Üí Environments
   - Create/edit the `copilot` environment

2. **Add Required Variables**
   ```bash
   # OpenMP Library Conflict Resolution
   KMP_DUPLICATE_LIB_OK=TRUE
   
   # Threading Optimization
   OMP_NUM_THREADS=4
   
   # GPU Configuration (optional)
   CUDA_VISIBLE_DEVICES=0
   GPU_MEMORY_GB=12
   ```

### Workflow Verification

The `.github/workflows/copilot-setup-steps.yml` has been optimized for:
- ‚úÖ **UV installation** with proper PATH management
- ‚úÖ **CUDA PyTorch** installation from official index
- ‚úÖ **GPU dependencies** with CPU fallback
- ‚úÖ **Environment variable** propagation

#### Test GitHub Environment
```bash
# Check if workflow dependencies install correctly
cat .github/workflows/copilot-setup-steps.yml | grep -E "uv|torch|gpu"
```

---

## ‚ö° Performance Benchmarks

### GPU Acceleration Results (RTX 3060)
- **Overall Speed**: 19.4x real-time processing
- **Peak Performance**: 29.8x real-time  
- **Model Loading**: 0.7s (ultra-fast)
- **Memory Usage**: <2GB VRAM
- **Efficiency**: 94.8% time savings

### Scalability
- **Large files (>35KB)**: 23.0x average speed ‚≠ê
- **Medium files (25-35KB)**: 18.7x average speed  
- **Small files (<25KB)**: 13.2x average speed

---

## üß™ Testing

### Run Complete Test Suite
```bash
# TypeScript tests
pnpm test

# Python GPU tests (if available)
python tests/gpu_acceleration/test_gpu_final.py

# Benchmark tests
python tests/gpu_acceleration/benchmark_gpu_vs_cpu.py
```

### Manual Audio Test
```bash
# Test with sample audio file
python -c "
from faster_whisper import WhisperModel
import time
model = WhisperModel('tiny', device='cuda' if __import__('torch').cuda.is_available() else 'cpu')
print('Testing audio transcription...')
# Replace with actual audio file path
# segments, info = model.transcribe('path/to/audio.mp3')
print('Audio transcription ready!')
"
```

---

## üêõ Troubleshooting

### Common Issues & Solutions

#### 1. "uv command not found" in GitHub Actions
**Solution**: PATH export fixed in workflow
```yaml
export PATH="$HOME/.cargo/bin:$PATH"
```

#### 2. "OMP: Error #15: Initializing libiomp5md.dll"
**Solution**: Set environment variable
```bash
export KMP_DUPLICATE_LIB_OK=TRUE
```

#### 3. NumPy Compatibility Error
**Solution**: Downgrade NumPy
```bash
uv pip install "numpy<2.0.0"
```

#### 4. CUDA Not Available
**Solutions**:
```bash
# Check NVIDIA drivers
nvidia-smi

# Reinstall CUDA PyTorch
uv pip install torch==2.4.0+cu121 --index-url https://download.pytorch.org/whl/cu121 --force-reinstall
```

#### 5. faster-whisper GPU Fails
**Fallback**: Automatic CPU fallback is built-in
```python
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = WhisperModel('tiny', device=device)
```

---

## üìÅ Project Structure

```
Pdf2Md/
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îú‚îÄ‚îÄ workflows/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ copilot-setup-steps.yml    # ‚úÖ Fixed GitHub Copilot setup
‚îÇ   ‚îú‚îÄ‚îÄ SETUP_INSTRUCTIONS.md          # This file
‚îÇ   ‚îî‚îÄ‚îÄ copilot-instructions.md        # Development guidelines
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ tools.ts                       # üéØ MCP tools (ready for GPU integration)
‚îÇ   ‚îú‚îÄ‚îÄ utils.ts                       # Utilities
‚îÇ   ‚îî‚îÄ‚îÄ index.ts                       # Main server
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ gpu_acceleration/              # ‚úÖ Complete GPU test suite
‚îÇ   ‚îî‚îÄ‚îÄ audio_samples/                 # Test audio files
‚îú‚îÄ‚îÄ pyproject.toml                     # ‚úÖ Fixed Python dependencies
‚îî‚îÄ‚îÄ requirements-gpu.txt               # CUDA libraries reference
```

---

## üéØ Development Workflow

### For Contributors

1. **Setup Environment**
   ```bash
   git clone https://github.com/ArtemisAI/Pdf2Md.git
   cd Pdf2Md
   pnpm install
   uv sync --extra gpu
   ```

2. **GPU Integration Testing**
   ```bash
   # Test GPU functionality
   python tests/gpu_acceleration/test_gpu_final.py
   
   # Run benchmarks
   python tests/gpu_acceleration/benchmark_gpu_vs_cpu.py
   ```

3. **MCP Server Development**
   ```bash
   # Build and test
   pnpm run build
   pnpm test
   
   # Development mode
   pnpm run dev
   ```

### For GitHub Copilot Integration

The environment is fully configured for GitHub Copilot coding sessions with:
- ‚úÖ All dependencies auto-install via workflow
- ‚úÖ GPU acceleration ready (19.4x performance)
- ‚úÖ CPU fallback for universal compatibility
- ‚úÖ Environment variables configured

---

## üìä Success Metrics

### Installation Verification Checklist
- [ ] Node.js dependencies installed (`pnpm install`)
- [ ] Python base dependencies installed (`uv sync`)  
- [ ] GPU dependencies installed (`uv sync --extra gpu`)
- [ ] CUDA PyTorch working (`torch.cuda.is_available() == True`)
- [ ] faster-whisper GPU ready (`WhisperModel('tiny', device='cuda')`)
- [ ] MCP server builds (`pnpm run build`)
- [ ] GitHub environment variables set

### Performance Targets
- [ ] **GPU Speed**: >15x real-time (target: 19.4x achieved)
- [ ] **Model Loading**: <2s (target: 0.7s achieved)
- [ ] **CPU Fallback**: >2x real-time 
- [ ] **Memory Usage**: <2GB VRAM

---

## üÜò Support

### Documentation Resources
- **GPU Enhancement**: `docs/audio_enhancement/GPU_ACCELERATION_REPORT.md`
- **Testing Guide**: `docs/audio_enhancement/TESTING_PROTOCOL.md`
- **Status Overview**: `.github/AUDIO_ENHANCEMENT_STATUS.md`

### Contact & Issues
- **GitHub Issues**: [Report problems](https://github.com/ArtemisAI/Pdf2Md/issues)
- **Discussions**: [Community support](https://github.com/ArtemisAI/Pdf2Md/discussions)

---

**Last Updated**: December 2024  
**GPU Performance**: 19.4x real-time on RTX 3060  
**Status**: Production ready with GitHub Copilot support ‚úÖ
