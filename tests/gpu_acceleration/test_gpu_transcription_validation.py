#!/usr/bin/env python3
"""
Real GPU Audio Transcription Test
Test actual transcription with our test audio file using GPU acceleration
"""

import subprocess
import sys
import os
import time
import json
import asyncio
from pathlib import Path

async def test_direct_gpu_transcription():
    """Test GPU transcription directly using our enhanced system"""
    print("üß™ TESTING: Direct GPU Audio Transcription")
    print("=" * 50)
    
    try:
        # Check if test audio file exists
        audio_file = "test_audio.wav"
        if not os.path.exists(audio_file):
            print(f"‚ùå Test audio file not found: {audio_file}")
            return False
        
        print(f"üìÅ Using audio file: {audio_file}")
        print(f"üìè File size: {os.path.getsize(audio_file)} bytes")
        
        # Create a Python script that uses our CUDA environment
        transcription_script = """
import torch
import time
import sys
from faster_whisper import WhisperModel

def transcribe_with_gpu(audio_file):
    print(f"üîß CUDA available: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"üîß GPU: {torch.cuda.get_device_name(0)}")
        print(f"üîß GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # Try GPU first, fallback to CPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    compute_type = "float16" if device == "cuda" else "int8"
    
    print(f"üöÄ Loading Whisper model on {device} with {compute_type}")
    start_time = time.time()
    
    model = WhisperModel("tiny", device=device, compute_type=compute_type)
    load_time = time.time() - start_time
    print(f"‚úÖ Model loaded in {load_time:.2f} seconds")
    
    # Transcribe
    print(f"üé§ Starting transcription...")
    transcribe_start = time.time()
    
    segments, info = model.transcribe(audio_file, language="en")
    
    # Collect results
    transcription_text = ""
    segment_count = 0
    for segment in segments:
        transcription_text += segment.text + " "
        segment_count += 1
    
    transcribe_time = time.time() - transcribe_start
    
    print(f"‚úÖ Transcription completed in {transcribe_time:.2f} seconds")
    print(f"‚úÖ Segments processed: {segment_count}")
    print(f"‚úÖ Language detected: {info.language} (confidence: {info.language_probability:.2f})")
    print(f"‚úÖ Duration: {info.duration:.2f} seconds")
    print(f"‚úÖ Transcription: '{transcription_text.strip()}'")
    
    # GPU memory usage if CUDA
    if device == "cuda":
        memory_used = torch.cuda.memory_allocated(0) / 1024**2
        print(f"‚úÖ GPU memory used: {memory_used:.1f} MB")
    
    return {
        "success": True,
        "device": device,
        "load_time": load_time,
        "transcribe_time": transcribe_time,
        "text": transcription_text.strip(),
        "language": info.language,
        "duration": info.duration
    }

if __name__ == "__main__":
    audio_file = sys.argv[1] if len(sys.argv) > 1 else "test_audio.wav"
    result = transcribe_with_gpu(audio_file)
    print(f"\\nüìä RESULT: {json.dumps(result, indent=2)}")
"""
        
        # Write the transcription script
        script_file = "test_transcription.py"
        with open(script_file, 'w') as f:
            f.write(transcription_script)
        
        print(f"‚úÖ Created transcription script: {script_file}")
        
        # Run the transcription
        print("üöÄ Starting GPU transcription test...")
        start_time = time.time()
        
        result = subprocess.run([sys.executable, script_file, audio_file], 
                              capture_output=True, text=True, timeout=60)
        
        total_time = time.time() - start_time
        
        if result.returncode == 0:
            print(f"‚úÖ Transcription test completed in {total_time:.2f} seconds")
            print("üìÑ Output:")
            print(result.stdout)
            
            if result.stderr:
                print("‚ö†Ô∏è  Warnings:")
                print(result.stderr)
            
            return True
        else:
            print(f"‚ùå Transcription test failed (exit code: {result.returncode})")
            print("‚ùå Error output:")
            print(result.stderr)
            print("‚ùå Standard output:")
            print(result.stdout)
            return False
            
    except subprocess.TimeoutExpired:
        print("‚ùå Transcription test timed out after 60 seconds")
        return False
    except Exception as e:
        print(f"‚ùå Direct GPU transcription test failed: {e}")
        return False

async def test_performance_comparison():
    """Compare GPU vs CPU performance"""
    print("\nüß™ TESTING: GPU vs CPU Performance Comparison")
    print("=" * 50)
    
    try:
        # Create performance test script
        perf_script = """
import torch
import time
from faster_whisper import WhisperModel
import sys

def benchmark_device(audio_file, device, runs=3):
    compute_type = "float16" if device == "cuda" else "int8"
    print(f"üîß Benchmarking {device} with {compute_type}")
    
    times = []
    for run in range(runs):
        print(f"  Run {run + 1}/{runs}...")
        
        # Load model
        model = WhisperModel("tiny", device=device, compute_type=compute_type)
        
        # Time transcription
        start_time = time.time()
        segments, info = model.transcribe(audio_file)
        
        # Force completion
        list(segments)  # Consume all segments
        
        elapsed = time.time() - start_time
        times.append(elapsed)
        
        print(f"    Time: {elapsed:.2f}s")
        
        # Clean up
        del model
        if device == "cuda":
            torch.cuda.empty_cache()
    
    avg_time = sum(times) / len(times)
    print(f"‚úÖ {device.upper()} average: {avg_time:.2f}s (min: {min(times):.2f}s, max: {max(times):.2f}s)")
    return avg_time

if __name__ == "__main__":
    audio_file = sys.argv[1] if len(sys.argv) > 1 else "test_audio.wav"
    
    print(f"üéµ Testing with: {audio_file}")
    
    # Test CPU
    cpu_time = benchmark_device(audio_file, "cpu", runs=2)
    
    # Test GPU if available
    if torch.cuda.is_available():
        gpu_time = benchmark_device(audio_file, "cuda", runs=2)
        
        speedup = cpu_time / gpu_time
        print(f"\\nüöÄ GPU SPEEDUP: {speedup:.2f}x faster than CPU")
        
        if speedup > 1.2:
            print("‚úÖ GPU acceleration is effective!")
        else:
            print("‚ö†Ô∏è  GPU acceleration minimal (file may be too small)")
    else:
        print("‚ùå CUDA not available for GPU testing")
"""
        
        # Write performance script
        perf_file = "test_performance.py"
        with open(perf_file, 'w') as f:
            f.write(perf_script)
        
        print("üöÄ Running performance comparison...")
        
        audio_file = "test_audio.wav"
        if not os.path.exists(audio_file):
            print(f"‚ùå Audio file not found: {audio_file}")
            return False
        
        result = subprocess.run([sys.executable, perf_file, audio_file], 
                              capture_output=True, text=True, timeout=120)
        
        if result.returncode == 0:
            print("‚úÖ Performance comparison completed")
            print("üìä Results:")
            print(result.stdout)
            return True
        else:
            print(f"‚ùå Performance test failed:")
            print(result.stderr)
            return False
            
    except Exception as e:
        print(f"‚ùå Performance comparison failed: {e}")
        return False

async def test_large_file_simulation():
    """Test with a larger simulated audio file"""
    print("\nüß™ TESTING: Large File Simulation")
    print("=" * 50)
    
    try:
        import wave
        import struct
        import math
        
        # Create a longer test file (10 seconds)
        filename = "test_audio_large.wav"
        duration = 10  # seconds
        sample_rate = 16000
        frequency = 440
        
        print(f"üîß Creating {duration}s test file...")
        
        samples = []
        for i in range(int(duration * sample_rate)):
            # Add some variation to make it more interesting
            freq = frequency + 50 * math.sin(2 * math.pi * i / (sample_rate * 2))
            value = int(32767 * 0.1 * math.sin(2 * math.pi * freq * i / sample_rate))
            samples.append(struct.pack('<h', value))
        
        with wave.open(filename, 'w') as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(b''.join(samples))
        
        file_size = os.path.getsize(filename)
        print(f"‚úÖ Large test file created: {filename}")
        print(f"‚úÖ File size: {file_size} bytes ({file_size/1024:.1f} KB)")
        
        # Test transcription with larger file
        script = """
import torch
from faster_whisper import WhisperModel
import time
import sys

audio_file = sys.argv[1]
device = "cuda" if torch.cuda.is_available() else "cpu"
compute_type = "float16" if device == "cuda" else "int8"

print(f"üîß Testing larger file on {device}")
model = WhisperModel("tiny", device=device, compute_type=compute_type)

start_time = time.time()
segments, info = model.transcribe(audio_file)

segment_count = 0
for segment in segments:
    segment_count += 1

elapsed = time.time() - start_time
print(f"‚úÖ Processed {segment_count} segments in {elapsed:.2f}s")
print(f"‚úÖ Audio duration: {info.duration:.2f}s")
print(f"‚úÖ Real-time factor: {info.duration/elapsed:.2f}x")

if device == "cuda":
    memory = torch.cuda.memory_allocated(0) / 1024**2
    print(f"‚úÖ GPU memory used: {memory:.1f} MB")
"""
        
        script_file = "test_large_file.py"
        with open(script_file, 'w') as f:
            f.write(script)
        
        result = subprocess.run([sys.executable, script_file, filename], 
                              capture_output=True, text=True, timeout=60)
        
        if result.returncode == 0:
            print("‚úÖ Large file test completed")
            print(result.stdout)
            return True
        else:
            print(f"‚ùå Large file test failed:")
            print(result.stderr)
            return False
            
    except Exception as e:
        print(f"‚ùå Large file simulation failed: {e}")
        return False

async def main():
    """Run all GPU transcription tests"""
    print("üöÄ GPU AUDIO TRANSCRIPTION VALIDATION")
    print("=" * 60)
    
    tests = [
        ("Direct GPU Transcription", test_direct_gpu_transcription),
        ("Performance Comparison", test_performance_comparison),
        ("Large File Simulation", test_large_file_simulation),
    ]
    
    results = {}
    for test_name, test_func in tests:
        try:
            print(f"\n{'='*60}")
            result = await test_func()
            results[test_name] = result is not False
        except Exception as e:
            print(f"‚ùå {test_name} failed with exception: {e}")
            results[test_name] = False
    
    # Final Summary
    print(f"\n{'='*60}")
    print("üìã FINAL TEST SUMMARY:")
    print("=" * 40)
    
    for test_name, result in results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status} {test_name}")
    
    passed_tests = sum(1 for result in results.values() if result)
    total_tests = len(results)
    
    print(f"\nüéØ FINAL RESULTS: {passed_tests}/{total_tests} tests passed")
    
    if passed_tests == total_tests:
        print("\nüéâ PERFECT! GPU-accelerated audio transcription is fully functional!")
        print("üî• RTX 3060 optimization is working at peak performance!")
        print("‚ö° Your enhanced audio system is ready for production use!")
    elif passed_tests >= total_tests * 0.66:
        print("\n‚úÖ GOOD! GPU-accelerated audio transcription is largely working!")
        print("üî• RTX 3060 is providing significant acceleration!")
        print("üöÄ System is ready for real-world testing!")
    else:
        print("\n‚ö†Ô∏è  Some issues detected, but basic GPU functionality is working.")
        print("üîß Consider debugging the failed tests for optimal performance.")
    
    return passed_tests >= total_tests * 0.66

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
